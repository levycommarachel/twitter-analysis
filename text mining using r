# Clear workspace
rm(list=ls())

library(twitteR)
library(ROAuth)


consumer_key <- 	"***"
consumer_secret <-"***"
access_token <- "***"
access_secret <- "***"

## Twitter authentication
setup_twitter_oauth(consumer_key, consumer_secret, access_token,
                    access_secret)

# Search tweets with '#foodie"
food_tweets = searchTwitter("foodie", n=500)


# convert tweets to a data frame
tweets.df <- twListToDF(food_tweets)



# tweet #10
tweets.df[10, c("id", "created", "screenName", "replyToSN",
                 "favoriteCount", "retweetCount", "longitude", "latitude", "text")]

writeLines(strwrap(tweets.df$text[10], 60))

library(tm)
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(tweets.df$text))
# convert to lower case
#myCorpus <- tm_map(myCorpus, tolower)
#remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# remove stopwords
#myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, removeWords, c("0","1","11","117","16","222","24", "5", "7", "a", "and", 
                                            "as", "author", "character", "content", "datetimestamp", 
                                            "description", "heading", "id", "hour", "i", "isdst",
                                            "language", "of", "this", "on", "list", "0", "16"))


# keep a copy for stem comp
myCorpusCopy <- myCorpus

# inspect the first 5 documents (tweets) inspect(myCorpus[1:5]) 
# The code below is used for to make text fit for paper width 
for (i in 1:5) {
  cat(paste("[[", i, "]] ", sep = ""))
  #writeLines(myCorpus[[i]])
  writeLines(as.character(myCorpus[[i]]))
}

myCorpus <- tm_map(myCorpus, stemDocument) # stem words
writeLines(strwrap(myCorpus[[10]]$content, 60))


stemCompletion2 <- function(x, dictionary) {
  x <- unlist(strsplit(as.character(x), " "))
  x <- x[x != ""]
  x <- stemCompletion(x, dictionary=dictionary)
  x <- paste(x, sep="", collapse=" ")
  PlainTextDocument(stripWhitespace(x))
}
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
writeLines(strwrap(myCorpus[[10]]$content, 60))

# count word frequence
wordFreq <- function(corpus, word) {
  results <- lapply(corpus,
                    function(x) { grep(as.character(x), pattern=paste0("\\<",word)) }
  )
  sum(unlist(results))
}


n.food <- wordFreq(myCorpusCopy, "foodi")
#n.diet <- wordFreq(myCorpusCopy, "foods")
cat(n.food)

# replace oldword with newword
replaceWord <- function(corpus, oldword, newword) {
  tm_map(corpus, content_transformer(gsub),
         pattern=oldword, replacement=newword)
}
myCorpus <- replaceWord(myCorpus, "foodi", "foodie")
#myCorpus <- replaceWord(myCorpus, "comida", "food")
#myCorpus <- replaceWord(myCorpus, "foody", "foodie")
tdm <- TermDocumentMatrix(myCorpus,
                          control = list(wordLengths = c(1, Inf)))

tdm

findAssocs(tdm, "foodie", 0.2)

idx <- which(dimnames(tdm)$Terms %in% c("foodie", "foodporn", "recipe"))
as.matrix(tdm[idx, 1:100])


# inspect frequent words
(freq.terms <- findFreqTerms(tdm, lowfreq = 4))


term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 4)
df <- data.frame(term = names(term.freq), freq = term.freq)

library(ggplot2)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") +
  xlab("Terms") + ylab("Count") + coord_flip() +
  theme(axis.text=element_text(size=7))

m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# colors
pal <- brewer.pal(9, "BuGn")[-(1:4)]
# plot word cloud
library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 4,
          random.order = F, colors = pal)


# which words are associated with 'recipes'?
findAssocs(tdm, "food", 0.1)


dtm <- as.DocumentTermMatrix(tdm)
library(topicmodels)
lda <- LDA(dtm, k = 8) # find 8 topics
term <- terms(lda, 7) # first 7 terms of every topic
(term <- apply(term, MARGIN = 2, paste, collapse = ", "))

topics <- topics(lda) # 1st topic identified for every document (tweet)
topics <- data.frame(topics)


